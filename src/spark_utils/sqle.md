Spark SQL make easy
===================

Spark-SQL is awesome and I kindly wanted to use it for everything. Well probably not everything, but for a lot of things of BigData reports. ;) .

sqle is a python command line tool that intended to make Spark-SQL easy.

Let say:

1) you have a file ```table.sql```. For people who familiar with python might already notices that there are some template variables in the file.

```SQL
CREATE temporary table TBL_A
USING com.databricks.spark.csv
OPTIONS (path "{A}", header "true");

CREATE temporary table TBL_B
USING com.databricks.spark.csv
OPTIONS (path "{B}", header "true");

```

2) you want to run a query and you want to specify template variable ```{A}``` and ```{B}``` to
```s3://mybk/a``` and ```s3://mybk/b```.

3) and you want to run a SQL query like:

```SQL
SELECT * from TBL_A INNER JOIN TBL_B on TBL_A.id = TBL_B.id
```

With ```sqle```, you can simply do:

```BASH
sqle.py table.sql "SELECT * from TBL_A INNER JOIN TBL_B on TBL_A.id = TBL_B.id" \
  -x A=s3://mybk/a B=s3://mybk/b \
  --dry
```

It will output to STDOUT as:

```SQL
-- dry run, will only output SQL to stdout.
-- generated by sqle (https://github.com/tly1980/spark_utils)
-- stmt_0, src: table.sql
CREATE temporary table TBL_A
USING com.databricks.spark.csv
OPTIONS (path "s3://mybk/a", header "true");

-- stmt_1, src: table.sql
CREATE temporary table TBL_B
USING com.databricks.spark.csv
OPTIONS (path "s3://mybk/b", header "true");

-- stmt_2, src: <FROM ARGS>
SELECT * from TBL_A INNER JOIN TBL_B on TBL_A.id = TBL_B.id;
```

You can specify multiple sql files.
```BASH
sqle.py a.sql b.sql c.sql -x A=s3://mybk/a B=s3://mybk/b --dry
```

Or you could simply specify the dir for src, the ```sqle.py``` will recursively list all files under the directory and compile and render them into one SQL. 


If you are happy with the SQL, you can remove the ```--dry``` and simply use spark-submit with sqle.

For example:

```
spark-submit sqle.py table.sql 'SELECT * from TBL_A INNER JOIN TBL_B on TBL_A.id = TBL_B.id'\
  -x A=s3://mybk/a B=s3://mybk/b
```


Usage
-----

```
usage: sqle.py [-h] [-x X [X ...]] [--quiet] [--dry] [--nohive] src [src ...]

Spark SQL enhancer / template render

positional arguments:
  src

optional arguments:
  -h, --help    show this help message and exit
  -x X [X ...]  Specify the template variables. Please put it as "-x a=AAA
                b=BBB c=CCC"
  --quiet       Not show
  --dry         Dry run
  --nohive      Use SqlContext rather than HiveContext.
```
